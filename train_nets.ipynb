{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4hXrBeiZ469"
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from pybedtools import BedTool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, \\\n",
    "    recall_score\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y_VvC501Z47F",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import Sampler, BatchSampler\n",
    "from torch.nn.modules.loss import MSELoss\n",
    "\n",
    "torch.manual_seed(61406)\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {\n",
    "    'A': [1, 0, 0, 0],\n",
    "    'C': [0, 1, 0, 0],\n",
    "    'G': [0, 0, 1, 0],\n",
    "    'T': [0, 0, 0, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uOmCNVu6Z47c"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class G4HIDataset(Dataset):  # G4 & histones intersection\n",
    "    def __init__(self, pos_filename, neg_filename):\n",
    "        self.data = [record.seq.upper() for record in SeqIO.parse(pos_filename, 'fasta')\n",
    "                     if 'N' not in record.seq.upper()]\n",
    "        pos_size = len(self.data)\n",
    "        self.data.extend([record.seq.upper() for record in SeqIO.parse(neg_filename, 'fasta')\n",
    "                     if 'N' not in record.seq.upper()])\n",
    "        neg_size = len(self.data) - pos_size\n",
    "        self.labels = torch.unsqueeze(\n",
    "            torch.FloatTensor(\n",
    "                torch.cat((torch.ones(pos_size), torch.zeros(neg_size)), dim=0)\n",
    "            ), 1)\n",
    "        torch.manual_seed(7642)\n",
    "        self.indexes = torch.randperm(len(self.data))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (torch.FloatTensor([label_dict[bp] for bp in self.data[self.indexes[index]]]),\n",
    "                self.labels[self.indexes[index]])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_names = [fn[2:-4] for fn in os.listdir('histone_modifications/') if fn[:2] == 'i_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "bfDzlNXqZ470",
    "outputId": "58a29bd2-5cac-44eb-caf9-7f38056bc988",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def negative_class(train_num, test_num):\n",
    "    f_tr_out = open(f'train_negative', 'w')\n",
    "    f_ts_out = open(f'test_negative', 'w')\n",
    "    for i, seq_record in enumerate(SeqIO.parse('negative_rand.fa', 'fasta')):\n",
    "        if 'N' not in seq_record.seq.upper():\n",
    "            if i < train_num:\n",
    "                SeqIO.write(seq_record, f_tr_out, 'fasta')\n",
    "            elif i < train_num + test_num:\n",
    "                SeqIO.write(seq_record, f_ts_out, 'fasta')\n",
    "            else:\n",
    "                break\n",
    "    f_tr_out.close()\n",
    "    f_ts_out.close()\n",
    "\n",
    "def make_dataset(hist_name):\n",
    "    print(f'{hist_name}  (making dataset)')\n",
    "    \n",
    "    test_prop = 0.7  # proportion of test data\n",
    "    # positive class\n",
    "    data_bed = BedTool(f'histone_modifications/pos_i_{hist_name}.bed')\n",
    "    train_size = int(len(data_bed) * test_prop)\n",
    "    train_bed = BedTool(list(data_bed[: train_size]))\n",
    "    test_bed = BedTool(list(data_bed[train_size:]))\n",
    "    \n",
    "    test_bed_iv = test_bed.intersect(b=train_bed, v=True)\n",
    "    test_size = len(test_bed_iv)\n",
    "    \n",
    "    # теперь получаем fasta файлы\n",
    "    train_seq = train_bed.sequence(fi='hg19.fa')\n",
    "    test_seq = test_bed_iv.sequence(fi='hg19.fa')\n",
    "    \n",
    "    # negative class\n",
    "    quad_bed = BedTool(f'quad_centered_500.bed')\n",
    "    quad_bed = quad_bed.intersect(b=train_bed, v=True)\n",
    "    quad_bed_neg = quad_bed.intersect(b=test_bed_iv, v=True)\n",
    "    \n",
    "    quad_train_size = int(len(quad_bed_neg) * test_prop)\n",
    "    if quad_train_size >= train_size:\n",
    "        train_neg = list(quad_bed_neg[: train_size])\n",
    "        test_neg = list(quad_bed_neg[train_size: train_size + test_size])\n",
    "    else:\n",
    "        train_neg = list(quad_bed_neg[: quad_train_size])\n",
    "        test_neg = list(quad_bed_neg[quad_train_size: ])\n",
    "        with open('negative_rand.bed', 'r') as f:\n",
    "            neg_rand = ['\\t'.join(line.split('\\t')[:3]) + '\\n' for line in f.readlines()]\n",
    "\n",
    "        rand_tr_size = train_size - quad_train_size\n",
    "        train_neg.extend(neg_rand[: rand_tr_size])\n",
    "\n",
    "        if test_size - len(test_neg) > 0:\n",
    "            test_neg.extend(neg_rand[rand_tr_size: rand_tr_size + test_size - len(test_neg)])\n",
    "        else:\n",
    "            # when (size of test set) << (size of train set) * (test_prop) because of intersection\n",
    "            test_neg = test_neg[: test_size]\n",
    "        \n",
    "    train_bed_neg = BedTool(train_neg)\n",
    "    test_bed_neg = BedTool(test_neg)\n",
    "    test_bed_neg_iv = test_bed_neg.intersect(b=train_bed_neg, v=True)\n",
    "\n",
    "    train_seq_neg = train_bed_neg.sequence(fi='hg19.fa')\n",
    "    train_seq_neg.save_seqs(f'histone_modifications/{hist_name}_train_neg.fa')\n",
    "    \n",
    "    test_seq_neg = test_bed_neg_iv.sequence(fi='hg19.fa')\n",
    "    test_seq_neg.save_seqs(f'histone_modifications/{hist_name}_test_neg.fa')\n",
    "    \n",
    "    train_ratio = round(len(train_seq)/len(train_seq_neg), 4)\n",
    "    print(f'train sizes: {len(train_seq)}, {len(train_seq_neg)}, {train_ratio}% ratio')\n",
    "    test_ratio = round(len(test_seq)/len(test_seq_neg), 4)\n",
    "    print(f'test sizes: {len(test_seq)}, {len(test_seq_neg)}, {test_ratio}% ratio')\n",
    "    return train_seq.seqfn, train_seq_neg.seqfn, test_seq.seqfn, test_seq_neg.seqfn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Процедура обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    tr_loss_log = []\n",
    "    model.train()\n",
    "    pbar = tqdm_notebook(enumerate(train_loader))\n",
    "    for i, (data, target) in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.transpose(1, 2))\n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        pbar.set_description(f\"train loss: {round(loss, 7)}\")\n",
    "        tr_loss_log.append(loss)\n",
    "        if i > int(len(train_loader) * 0.8):\n",
    "            break\n",
    "    val_loss_log = []\n",
    "    model.eval()\n",
    "    for i, (data, target) in pbar:\n",
    "        output = model(data.transpose(1, 2))\n",
    "        loss = F.binary_cross_entropy(output, target)\n",
    "        loss = loss.item()\n",
    "        val_loss_log.append(loss)\n",
    "        pbar.set_description(f\"val loss: {round(loss, 7)}\")\n",
    "    return tr_loss_log, val_loss_log\n",
    "\n",
    "def train(model, opt, model_name='model'):\n",
    "    train_log = []\n",
    "    val_log = [(0, 1)]\n",
    "\n",
    "    epoch = 0\n",
    "    while len(val_log) == 1 or val_log[-2][1] - val_log[-1][1] > 0.0001:\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        train_loss, val_loss = train_epoch(model, opt)\n",
    "        torch.save(model.state_dict(), f\"models/{model_name}_epoch_{epoch}.weights\")\n",
    "        train_log.extend(train_loss)\n",
    "        val_log.append((int(len(train_loader) * 0.8) * (epoch + 1), np.mean(val_loss)))\n",
    "        plot_history(train_log, val_log, model_name)\n",
    "        epoch += 1\n",
    "\n",
    "def plot_history(train_history, val_history, model_name, title='loss'):\n",
    "    plt.figure()\n",
    "    plt.title(f'{title}')\n",
    "    plt.plot(train_history, label='train', zorder=1)    \n",
    "    points = np.array(val_history)\n",
    "    plt.scatter(points[:, 0], points[:, 1], marker='+', s=180, c='orange', label='val', zorder=2)\n",
    "    plt.xlabel('steps')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    plt.savefig(f'training plots/{model_name}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size()[0], -1)\n",
    "\n",
    "class CNN_one_l(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(4, 16, 8),\n",
    "            nn.MaxPool1d(493, stride=493),\n",
    "            nn.ReLU(),\n",
    "            Flatten(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.cnn(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CNN_one_l()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hist_name in hist_names:\n",
    "    train_pos_fn, train_neg_fn, test_pos_fn, test_neg_fn = make_dataset(hist_name)\n",
    "    ghi_train = G4HIDataset(train_pos_fn, train_neg_fn)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=ghi_train, batch_size=batch_size)\n",
    "    \n",
    "    net = CNN_one_l()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    train(net, opt, f\"{hist_name}_CNN_one_layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'H3K4me3_(@_Wharton_Jelly)' in hist_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H3K4me3_(@_Wharton_Jelly)  (making dataset)\n",
      "train sizes: 103265, 103265, 1.0% ratio\n",
      "test sizes: 28773, 28772, 1.0% ratio\n"
     ]
    }
   ],
   "source": [
    "train_pos_fn, train_neg_fn, test_pos_fn, test_neg_fn = make_dataset('H3K4me3_(@_Wharton_Jelly)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghi_train = G4HIDataset(train_pos_fn, train_neg_fn)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=ghi_train, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i, (x, y) in enumerate(train_loader):\n",
    "    X_train.append(x.view(-1).int().tolist())\n",
    "    y_train.extend(y.view(-1).int().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=50448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 45s, sys: 1.52 s, total: 4min 46s\n",
      "Wall time: 4min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=50448,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghi_test = G4HIDataset(test_pos_fn, test_neg_fn)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=ghi_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    X_test.append(x.view(-1).int().tolist())\n",
    "    y_test.extend(y.view(-1).int().tolist())\n",
    "y_test = np.array(y_test)\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC ROC: 0.8538\n",
      "auc pr: 0.9023\n",
      "accuracy: 0.8539\n"
     ]
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)\n",
    "roc_auc = round(metrics.auc(fpr, tpr), 4)\n",
    "print('AUC ROC:', roc_auc)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred, pos_label=1)\n",
    "auc_pr = round(metrics.auc(recall, precision), 4)\n",
    "print(f'auc pr: {auc_pr}')\n",
    "\n",
    "acc = round(np.sum(y_test == y_pred) / len(y_test), 4)\n",
    "print(f'accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Визуализация фильтров и проверка качества на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict_rev = {\n",
    "    0: 'A',\n",
    "    1: 'C',\n",
    "    2: 'G',\n",
    "    3: 'T',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for hist_name in hist_names:\n",
    "    model_nms = [[int(el[:-8].split('_')[-1]), el] for el in os.listdir('models')\n",
    "                 if 'CNN_one_layer_epoch' in el and hist_name in el]\n",
    "    if len(model_nms) >= 2:\n",
    "        model_nm = sorted(model_nms)[-2][1]\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    train_pos_fn, train_neg_fn, test_pos_fn, test_neg_fn = make_dataset(hist_name)\n",
    "    ghi_test = G4HIDataset(test_pos_fn, test_neg_fn)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=ghi_test, batch_size=batch_size)\n",
    "    \n",
    "    net.load_state_dict(torch.load(f\"models/{model_nm}\"))\n",
    "    net.eval()\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "    for batch, target in test_loader:\n",
    "        with torch.no_grad():\n",
    "            y_pred.extend(net(batch.transpose(1, 2)))\n",
    "        y_test.extend(target)\n",
    "    y_pred_bool = torch.FloatTensor(y_pred) > 0.5\n",
    "    y_test = torch.FloatTensor(y_test)\n",
    "    print(f'epoch {sorted(model_nms)[-2][0]}:')\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1)\n",
    "    roc_auc = round(metrics.auc(fpr, tpr), 4)\n",
    "    print('AUC ROC:', roc_auc)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_pred, pos_label=1)\n",
    "    auc_pr = round(metrics.auc(recall, precision), 4)\n",
    "    print(f'auc pr: {auc_pr}')\n",
    "\n",
    "    acc = round(int((y_test == y_pred_bool).sum()) / y_test.shape[0], 4)\n",
    "    print(f'accuracy: {acc}')\n",
    "\n",
    "    plt.plot(fpr, tpr, color='darkorange', label=f'ROC AUC={roc_auc}')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.fill_between(fpr, tpr, color=(1.0, 0.5490196078431373, 0.0, 0.1))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title(f\"{hist_name.replace('@_', '').replace('_', ' ')}\")\n",
    "    plt.grid()\n",
    "    plt.savefig(f'ROC_curves/{hist_name}.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(recall, precision, label=f'PR AUC={auc_pr}')\n",
    "    plt.fill_between(recall, precision, color=(0,0,1,0.1))\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title(f\"{hist_name.replace('@_', '').replace('_', ' ')}\")\n",
    "    plt.grid()\n",
    "    plt.savefig(f'PR_curves/{hist_name}.png')\n",
    "    plt.show()\n",
    "\n",
    "    for i, filt in enumerate(net.state_dict()['cnn.0.weight']):\n",
    "        value = np.transpose(filt).max(axis=1).values.sum() + net.state_dict()['cnn.0.bias'][i]\n",
    "        s = ''.join([label_dict_rev[int(ind)] for ind in np.transpose(filt).max(axis=1).indices])\n",
    "\n",
    "        print(i, s, round(float(value * net.state_dict()['cnn.4.weight'][0][i]), 4))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
